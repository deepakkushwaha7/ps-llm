# ğŸ§  Problem solving LLM â€“ End-to-End Single-File Architecture (Colab)

This project demonstrates a **complete end-to-end pipeline** for building a **coding-focused Large Language Model (LLM)** using **one single Python file** in **Google Colab**.

The file contains **everything**:

* Dependency installation
* Dataset loading
* Data preprocessing
* Tokenization
* Model loading
* LoRA fine-tuning
* Model saving
* Inference-only interactive CLI

This design is intentional to make the full lifecycle easy to understand and reproduce.

---

## ğŸ“Œ High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Single Python File        â”‚
â”‚  (Colab Notebook Cell)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Dependency Installation    â”‚
â”‚ transformers, datasets,    â”‚
â”‚ peft, accelerate, bnb      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Dataset Loading            â”‚
â”‚ deepmind/code_contests     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data Preprocessing         â”‚
â”‚ â€¢ Python-only filtering    â”‚
â”‚ â€¢ Prompt formatting        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tokenization + Labels      â”‚
â”‚ â€¢ input_ids                â”‚
â”‚ â€¢ attention_mask           â”‚
â”‚ â€¢ labels = input_ids       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Base Model (4-bit)         â”‚
â”‚ DeepSeek-Coder 1.3B        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LoRA Fine-Tuning           â”‚
â”‚ â€¢ q_proj, v_proj adapters  â”‚
â”‚ â€¢ <1% trainable params     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Hugging Face Trainer       â”‚
â”‚ â€¢ Causal LM objective      â”‚
â”‚ â€¢ FP16 + grad accumulation â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Saved Model Artifacts      â”‚
â”‚ /content/my-code-llm       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Inference-Only CLI         â”‚
â”‚ â€¢ Coding prompts only      â”‚
â”‚ â€¢ Clean code output        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‚ File Structure

There is **only one file**:

```
ps.py   (or a single Colab cell)
```

This file contains **all steps in sequence**.

---

## ğŸ§© Detailed Architecture Explanation

### 1ï¸âƒ£ Dependency Installation (Runtime Layer)

The script installs all required libraries at runtime to ensure it works in a **fresh Colab session**.

---

### 2ï¸âƒ£ Dataset Layer

**Dataset:** `deepmind/code_contests`

* Competitive programming problems
* High-quality algorithmic solutions
* Language encoded using numeric IDs (`1 = Python`)

---

### 3ï¸âƒ£ Data Preprocessing Layer

Each problem is converted into an instruction-style prompt:

```text
### Problem:
<problem description>

### Write a Python solution.

### Solution:
<ground truth python code>
```

Only Python solutions are retained.

---

### 4ï¸âƒ£ Tokenization & Labeling

* Tokenizer reused from base model
* Max length: `512`
* Causal LM labels:

  ```python
  labels = input_ids
  ```

---

### 5ï¸âƒ£ Base Model Layer

**Model:** `deepseek-ai/deepseek-coder-1.3b-base`

* Loaded in **4-bit quantized mode**
* Optimized for low VRAM usage

---

### 6ï¸âƒ£ LoRA Fine-Tuning Layer

* Low-rank adapters on `q_proj` and `v_proj`
* ~0.23% trainable parameters
* Enables training on free Colab GPUs

---

### 7ï¸âƒ£ Training Layer

* Hugging Face `Trainer`
* Mixed precision (`fp16`)
* Gradient accumulation for stability

---

### 8ï¸âƒ£ Model Saving Layer

Artifacts saved to:

```
/content/my-code-llm
```

---

### 9ï¸âƒ£ Inference-Only CLI Layer

* Activated after training
* Rejects non-coding input
* Outputs clean Python code only

---

## â–¶ï¸ How to Run

### Step 1: Open Google Colab

Go to: [https://colab.research.google.com](https://colab.research.google.com)
Create a **new notebook**.

---

### Step 2: Enable GPU

In Colab menu:

```
Runtime â†’ Change runtime type â†’ Hardware accelerator â†’ GPU
```

---

### Step 3: Paste the Code

* Copy the **entire single-file script**
* Paste it into **one cell**
* Do **not split** into multiple files or cells

---

### Step 4: Run the Cell

Click **Run** and wait:

1. Dependencies install
2. Dataset downloads
3. Model loads
4. Training starts
5. Model is saved
6. CLI starts automatically

Training may take **30â€“60 minutes** depending on GPU availability.

---

### Step 5: Use the CLI

Once training finishes, you will see:

```
ğŸŸ¢ Input >
```

Example input:

```
Write a Python function to reverse a list
```

Type `exit` to quit the CLI.

---

## ğŸ§ª Example Interaction

````
ğŸŸ¢ Input > Write a Python function to check if a number is prime

ğŸ”µ Output >
```python
def is_prime(n):
    if n <= 1:
        return False
    for i in range(2, int(n**0.5) + 1):
        if n % i == 0:
            return False
    return True
````

---

## ğŸ› ï¸ Troubleshooting

### âŒ Dataset size becomes 0

**Cause:** Python solutions were not detected correctly.
**Fix:** Ensure language ID `1` is used for Python during preprocessing.

---

### âŒ `num_samples should be a positive integer`

**Cause:** Training dataset is empty.
**Fix:** Print dataset length after filtering:

```python
print(len(dataset))
```

---

### âŒ Training is very slow

**Cause:** Free Colab GPU limitations.
**Fixes:**

* Reduce dataset size (e.g. 2000 samples)
* Reduce `num_train_epochs`
* Be patient (expected behavior)

---

### âŒ Inference is slow

**Cause:** Large model + Colab latency.
**Fixes:**

* Use greedy decoding (`do_sample=False`)
* Reduce `max_new_tokens`
* Avoid interactive loops for long sessions

---

### âŒ Output is garbage or repetitive

**Cause:** Over-generation or sampling.
**Fix:**

* Reduce `max_new_tokens`
* Disable sampling

---

### âŒ CUDA Out of Memory

**Cause:** GPU memory exceeded.
**Fixes:**

* Reduce dataset size
* Ensure 4-bit loading is enabled
* Restart runtime and rerun

---

## ğŸ Summary

This single-file project demonstrates **real LLM engineering**:

> Dataset â†’ Preprocessing â†’ Tokenization â†’ Base Model â†’ LoRA â†’ Training â†’ CLI Inference

It is designed for **learning, experimentation, and architectural understanding**, not production deployment.


